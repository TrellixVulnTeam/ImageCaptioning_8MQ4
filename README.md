# Image-Captioning-PyTorch

*This repo contains codes to preprocess, train and evaluate sequence models on Flickr8k Image dataset in pytorch. This repo is a fork of "https://github.com/Subangkar/Image-Captioning-Attention-PyTorch".* 

 

Pretrained Resnet50 Resnext50 Res2net50 Inception-v3 and Res2next & LSTM with attention were added. And part of the code architecture was modified



**Pre-requisites**:

 - Datasets:

  - Flickr8k Dataset: [images](https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip) and [annotations](https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip)

 - Pre-trained word embeddings:

  - [Glove Embeddings of 6B words](http://nlp.stanford.edu/data/glove.6B.zip)



**Data Folder Structure for training using [`train_torch.py`](train_torch.py) or [`train_attntn.py`](train_attntn.py):**

**Train model with pretrained backbone by:**

```
python train_attntn.py-name [name of the model] -model [specified model]

e.g
python train_attntn.py --name 0503 --model resnet101_attention

```

where name represents the given name of the model and model represents type of model to be used in the training (including: resnet50_attention res2net50_attention resnext50_attention res2next50_attention) 

After training, model parameters of each epoch will be saved in ./saved_models for each model



**Besides, You can test model by:**

```
python test.py -name [name of the model] -model [specified model]

e.g.
python test.py --name 0503 --model resnet101_attention

```

where name and model has the exactly same meaning with train_attntn.py respectively. Notice that you should specify an exist model for testing.

After testing, captions generated by the model on testset will be saved in ./captions with the name format: name_model_captions



** TO visualize the attention map:**

```
python utils_plot.py -name [name of the model] -model [specified model]

e.g.
python utils_plot.py -name 0503 -model resnet101_attention
```

And the visualization results will be saved in ./visualization

You can modify the basic settting of training (e.g. learning rate) in config/global_config.yaml


```

data/

  flickr8k/

​    Flicker8k_Dataset/

​      *.jpg

​    Flickr8k_text/

​      Flickr8k.token.txt

​      Flickr_8k.devImages.txt

​      Flickr_8k.testImages.txt

​      Flickr_8k.trainImages.txt

  glove.6B/

​    glove.6B.50d.txt

​    glove.6B.100d.txt

​    glove.6B.200d.txt

​    glove.6B.300d.txt

```